---
title: "SYS 6018: Data Mining Final Project" 
author: "Lea Jih-Vieira, Mitch Whalen"
format: sys6018hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse) # functions for data manipulation  
library(ggplot2)
library(fastDummies)
library(ranger)
library(janitor)
library(e1071)
library(caret)
```

# Introduction

Many people enjoy mushroom foraging as a hobby and a lifestyle, hunting through woods to find ingredients for their next meal. However, foraging for mushrooms is not a casual activity by any means. Foraging requires vast knowledge of the wild mushrooms that grow in a given area and how to identify them ([Incredible Mushrooms](https://www.incrediblemushrooms.com/wild-mushroom-foraging.html)). It is imperative that a forager can tell the difference between an edible mushroom and a poisonous one, especially when the poisonous varieties can look almost identical to their edible counterparts ([Incredible Mushrooms](https://www.incrediblemushrooms.com/wild-mushroom-foraging.html)).

This report aims to provide foragers a model-based method of determining the probability that a foraged mushroom is poisonous or not. To accomplish this, we will utilize a dataset of macromorphological mushroom characteristics that a forager could observe without the need for more advanced technologies. By providing a model-based method to determine the probability of mushroom toxicity using easily observable features, we hope to provide foragers a more reliable and accurate method to identify mushrooms.

# Load Data

```{r, include = F}
df = read_csv2("/Users/mitchellwhalen/Desktop/GitHub/SYS6018FinalProject/secondary_data.csv", col_names = TRUE)
```

```{r}
head(df)
```

# Data Description

```{r}
# View the structure of the data
str(df)
```
Our data set has 61,069 instances of 21 variables.

*Response Variable:*
**class**: Binary class divided in edible=e and poisonous=p (with the latter one also containing mushrooms of unknown edibility).

*Predictor Variables:*
**cap-diameter**: Diameter of the mushroom cap in centimeters (quantitative).
**cap-shape**: Shape of the mushroom cap, categorized as bell, conical, convex, flat, sunken, spherical, or other (categorical).
**cap-surface**: Surface texture of the mushroom cap, categorized as fibrous, grooves, scaly, smooth, shiny, leathery, silky, sticky, wrinkled, fleshy, or other (categorical).
**cap-color**: Color of the mushroom cap, categorized as brown, buff, gray, green, pink, purple, red, white, yellow, blue, orange, black, or other (categorical).
**does-bruise-bleed**: Indicates whether the mushroom bruises or bleeds when damaged, categorized as bruises or bleeding (yes) or no (no) (categorical).
**gill-attachment**: Attachment of gills to the stem, categorized as adnate, adnexed, decurrent, free, sinuate, pores, none, or unknown (categorical).
**gill-spacing**: Spacing between gills, categorized as close, distant, or none (categorical).
**gill-color**: Color of the mushroom gills, similar to cap color, with additional category none (categorical).
**stem-height**: Height of the mushroom stem in centimeters (quantitative).
**stem-width**: Width of the mushroom stem in millimeters (quantitative).
**stem-root**: Root structure of the mushroom stem, categorized as bulbous, swollen, club, cup, equal, rhizomorphs, rooted, or unknown (categorical).
**stem-surface**: Surface texture of the mushroom stem, similar to cap surface, with additional category none (categorical).
**stem-color**: Color of the mushroom stem, similar to cap color, with additional category none (categorical).
**veil-type**: Type of veil covering the gills, categorized as partial or universal (categorical).
**veil-color**: Color of the veil, similar to cap color, with additional category none (categorical).
**has-ring**: Indicates whether the mushroom has a ring around the stem, categorized as ring (yes) or none (no) (categorical).
**ring-type**: Type of ring around the stem, categorized as cobwebby, evanescent, flaring, grooved, large, pendant, sheathing, zone, scaly, movable, none, or unknown (categorical).
**spore-print-color**: Color of the mushroom spore print, similar to cap color (categorical).
**habitat**: Habitat where the mushroom grows, categorized as grasses, leaves, meadows, paths, heaths, urban, waste, or woods (categorical).
**season**: Season in which the mushroom is typically found, categorized as spring, summer, autumn, or winter (categorical).

# Data Cleaning

## Missing Values

To begin with the data cleaning we will examine the data set by checking for missing values

```{r}
# Check for missing values
colSums(is.na(df))
```

Some of the columns (ex. stem-root) are missing a large percentage of values. Since it would be difficult to identify these features in a practical setting such as observing a mushroom in the wild, we will remove any columns that are missing more than 1/3 of the values for that variable. This ensures that the model is interpretable and the features deemed most important will be identifiable.

```{r}
# Threshold for removing columns with more than 50% missing values
threshold <- nrow(df) * 0.33

# Identify columns with missing values exceeding the threshold
missing_cols <- colSums(is.na(df)) > threshold

# Remove columns with more than 50% missing values
cleaned_df <- df[, !missing_cols]

colSums(is.na(cleaned_df))
```
For the remaining rows, given our abundance of data, we will just remove any rows that contain missing values. If we were to have a smaller dataset we may consider mode or predictive imputation to handle the missing values.

```{r}
# Remove rows with missing values
cleaned_df <- na.omit(cleaned_df)

colSums(is.na(cleaned_df))
```
## Quantitative Predictors

First, we must convert our quantitative variables to a numeric type

```{r}
# List of quantitative variable names
quantitative_vars <- c("cap-diameter", "stem-height", "stem-width")
# Add all the quantitative variable names you want to convert to this list

# Loop through each variable name and convert it to numeric
for (var in quantitative_vars) {
  cleaned_df[[var]] <- as.numeric(cleaned_df[[var]])
}

# Check the structure of your dataframe after conversion
str(cleaned_df)
```

Now we will visualize our 3 quantitative predictors to check for skewness, outliers, and other elements which could affect our ability to construct useful models.

```{r, echo=FALSE}
# Histogram for "cap-diameter"
ggplot(cleaned_df, aes(x = `cap-diameter`)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(x = "Cap Diameter", y = "Frequency", title = "Histogram of Cap Diameter")

# Histogram for "stem-height"
ggplot(cleaned_df, aes(x = `stem-height`)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(x = "Stem Height", y = "Frequency", title = "Histogram of Stem Height")

# Histogram for "stem-width"
ggplot(cleaned_df, aes(x = `stem-width`)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(x = "Stem Width", y = "Frequency", title = "Histogram of Stem Width")
```

These histograms show a clear skew in the data and some outliers in the upper tail. For certain models like regression models this could be impactful and is worth considering.

## Categorical Predictors

We will now visualize our categorical variables.

```{r, echo=FALSE}

# Function to create and display a bar plot for each logical and character variable
create_and_display_bar_plot <- function(df, var) {
  for (v in var) {
    if (is.logical(df[[v]]) || is.character(df[[v]])) {
      # Count frequency of each category
      freq_table <- table(df[[v]], useNA = "ifany")
      
      # Convert frequency table to dataframe
      freq_df <- as.data.frame(freq_table)
      names(freq_df) <- c("Category", "Frequency")
      
      # Create bar plot
      plot <- ggplot(freq_df, aes(x = Category, y = Frequency, fill = Category)) +
        geom_bar(stat = "identity") +
        labs(x = v, y = "Frequency", title = paste("Bar Plot of", v)) +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
      
      # Display plot
      print(plot)
    }
  }
}

# Create and display bar plots for logical and character variables
create_and_display_bar_plot(cleaned_df, names(cleaned_df))
```

One important note from these visualizations is that our class variable is roughly balanced, so we will not need to make adjustment for an imbalanced data set.

Next, we need to create dummy variables for the categorical variables. We will use one-hot encoding, so we can incorporate them into our model

```{r}
# Get logical and character variable names
logical_vars <- names(cleaned_df)[sapply(cleaned_df, is.logical)]
character_vars <- names(cleaned_df)[sapply(cleaned_df, is.character)]

# Perform one-hot encoding for logical and character variables
encoded_df <- dummy_cols(cleaned_df, select_columns = c(logical_vars, character_vars), remove_first_dummy = TRUE, remove_selected_columns = TRUE)

# Display the encoded dataframe
head(encoded_df)
```

# Model Development

## Train/Test Split

We will be implementing a 80/20 train/test split of the cleaned data. This will allow us to train the model on an adequate number of observations and also test our models on data it has not seen before.

```{r}
set.seed(2019)

train.idx = sample(nrow(encoded_df), # sample row indices
                   nrow(encoded_df)*0.8,# 80% of data
                   replace = FALSE)

Train = encoded_df[train.idx, ]
Train = clean_names(Train) # changes all - to _ in column names
Train$class_p = as.factor(Train$class_p)

Test = encoded_df[-train.idx, ]
Test = clean_names(Test) # changes all - to _ in column names
Test$class_p = as.factor(Test$class_p)
```

We will be implementing three different classification models: logistic regression, random forest, and support vector machine (SVM). In addition to these three models, we will be implementing an ensemble using up to these three models to see if the ensemble possesses better predictive power than the individual models.

## Model 1: Logistic Regression

We chose to use Logistic Regression as our first model because of its commonality to model relationships with binary dependent variables. Logistic Regression models are convenient because they are one of the easiest models to implement and train; however, they have more limitations and less flexibility compared to other classification models.

As we modeled in the data cleaning section, the numeric features of this data are heavily skewed and contain considerable outliers. Because this model type can be influenced by the presence of outliers, we chose to log transform those variables to lessen the effects of the potential outliers. Since we have observations where these values equal zero, we have to add a small constant to the numeric data to ensure we do not get values of infinity while log-transforming.

```{r}
num.features = c("cap_diameter", "stem_height", "stem_width")

Train.transformed = Train

for (column in num.features) {
  Train.transformed[ , column] = Train[ , column] + 2e-323 %>% log()
}
```

From here, we can fit a logistic regression model.

```{r, echo=FALSE}
fit_lr = glm(class_p ~ ., 
             family= 'binomial', 
             data= Train.transformed)
```

Using our logistic regression model, we can plot the probability estimates for the test data to observe the nature of the probability predictions.
```{r}
lr.p_hat= predict(fit_lr, Test, type= 'response') # response is the point estimate of the probability
```
```{r}
hist(lr.p_hat,
     main="Probability Estimates of Mushroom Toxicity",
     xlab="Probability Estimate",
     ylab = "Frequency",
     col="blue",
     border="pink",
     breaks = 30)
```
As we can see above, the model does not produce many "middle ground" probability estimates of mushroom toxicity. The estimates are almsost exclusively close to zero or with a couple very close to one. It is notable that the model predicts a lot more nontoxic classifications as opposed to toxic. This lack of ability to discriminate between poisonous and safe mushrooms makes the model virtually useless. Thus, we will try to reconstruct this model without transforming the quantitative predictors. Perhaps compressing the range of feature values excessively hampered the model's ability to discriminate.

```{r}
fit_lr = glm(class_p ~ ., 
             family= 'binomial', 
             data= Train)

lr.p_hat= predict(fit_lr, Test, type= 'response') # response is the point estimate of the probability

hist(lr.p_hat,
     main="Probability Estimates of Mushroom Toxicity",
     xlab="Probability Estimate",
     ylab = "Frequency",
     col="blue",
     border="pink",
     breaks = 30)
```

Based on this histogram we can see a more balanced set of predictions. Although the model clearly skews towards predicting the mushroom as poisonous, there appears to be at least some discriminatory ability now. We will evaluate this in the context of our other models later.

Next, we want to explore the feature importance of the variables in this model. Variables with larger coefficients are relied on the most for determining the probability of toxicity. A positive coefficent means the variable increases the likelihood the mushroom is poisonous, while a negative coefficient decreases the likelohood the mushroom is poisonous.

```{r}
# Extract coefficients from the fitted logistic regression model
coefficients <- coef(fit_lr)

# Create a data frame to store feature names and their corresponding coefficients
feature_importance <- data.frame(
  Feature = names(coefficients),
  Coefficient = coefficients
)

# Sort the data frame by the absolute value of the coefficients to identify the most important features
feature_importance <- feature_importance[order(abs(feature_importance$Coefficient), decreasing = TRUE),]

# Display the feature importance
head(feature_importance, 10)
```

Finding the feature importance is important for the application of this model in the real world. Based on this model, mushroom foragers could use these coefficients to begin to estimate whether they believe a mushroom is poisonous or not. For example, we can see that if stem_color_f = 1 (the mushroom has no stem), then it drastically increases the likelihood the mushroom is poisonous according to this model.

The features with the greatest importance in the logistic regression model were stem color and cap shape.

Finally, we can generate a preliminary accuracy score for this model using its hard classifications. Using a probability of 0.54 as our threshold (the guessing average), we can calculate the model's accuracy.

```{r}
lr.class = as.numeric(lr.p_hat > 0.54)

TP = sum(lr.class == 1 & lr.class == Test$class_p)
TN = sum(lr.class == 0 & lr.class == Test$class_p)

FP = sum(lr.class == 1 & lr.class != Test$class_p)
FN = sum(lr.class == 0 & lr.class != Test$class_p)

lr.accuracy = (TP + TN) / (TP + TN + FP + FN)

lr.accuracy
```
The model produces a preliminary accuracy of 0.78, which is a greater than the guessing average of 0.54 for a binary classification problem. Further analysis will be conducted in the model evaluation section later in this report.

## Model 2: Random Forest

The second model we wanted to explore is a tree-based model. Tree-based models are a powerful model to use because of their flexibility; they naturally include feature interactions and reduce the need of features transformations while still producing an intuitive model. Since we are trying to model a binary categorical outcome, we will need a classification tree that will allow us to estimate the probability of each class.

We chose to use a random forest model because it is generally known to be more robust to overfitting than its alternatives. The objective of this model is the minimize the loss function, which we will be using node impurity in our case. Node impurity refers to the probability for a random instance being misclassified when chosen randomly ([Towards Data Science](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c)). There are many metrics to measure node impurity, but we have chosen to use Gini Index in our model. Gini Index implements a soft approximation of the Misclassification Error impurity measure. This method is beneficial in situations where we want to prioritize accuracy, which is the case in our scenario. We want to be as accurate as possible in our mushroom toxicity predictions because even one mistake could be fatal to a forager.

We decided to run an out-of-bag analysis of the random forest to assess performance and identify the optimal tuning parameters. The default values from the `ranger()` function uses `mtry = sqrt(p) =` `r sqrt(ncol(Train)-1) %>% floor()` and `min.bucket=1`. Given the size of our data, however, we are restricted in the number of analyses and values we can test before computing power becomes a limitation. We ultimately decided to test `mtry` values between 4 and 16, and `min.bucket` values between 1 and 8. To evaluate the performance of each model, we used the `prediction.error` attribute from the model; for classification models, it produces the accuracy value of the model. In other words, the proportion of misclassified observation are reported by the `prediction.error` attribute.

```{r}
#: grid for tuning
tune_grid = expand_grid(
  mtry = seq(4, 16, by = 2),
  min.bucket = seq(1, 8, by = 2)
)

# OOB analysis
ACCURACY = tibble() # initiate results df
for(i in 1:nrow(tune_grid)) {
  mtry = tune_grid$mtry[i] # pull mtry value for this iteration
  
  min.bucket = tune_grid$min.bucket[i] # pull min-bucket value for this iteration
  
  rf = ranger(class_p~ ., 
              data = Train,
              mtry = mtry, # changes each iteration
              min.bucket = min.bucket, # changes each iteration
              num.trees = 500, # default
              splitrule = "gini", # gini index
              seed = 2021) # ensure same bagging for all mtry
  
  out = tibble(mtry, min.bucket,
               accuracy = rf$prediction.error)
  
  ACCURACY = bind_rows(ACCURACY, out)
}
```
```{r}
#-- Aggregate Results
ACCURACY_agg = ACCURACY %>% 
  group_by(mtry, min.bucket) %>% 
  summarize(n=n(), accuracy = accuracy, .groups="drop")
```
```{r}
#-- Plot Results
ACCURACY_agg %>% 
  mutate(min.bucket = factor(min.bucket)) %>% # make into factor for plotting
  ggplot(aes(mtry, accuracy, color=min.bucket)) + 
  geom_point() + geom_line()
```
Above we can see a visualization reporting the performance of each combination of `mtry` and `min.bucket` values run. This simulation has shown us that the default value `min.bucket = 1` and `mtry = 14` gives the best prediction for our data because it minimizes the prediction error the most.

Finally, we can test our model on the test set using our optimal tuning parameters.
```{r}
final.rf = ranger(class_p~ ., 
                  data = Train,
                  mtry = 14, # optimal mtry
                  min.bucket = 1, # optimal min.bucket
                  num.trees = 500, # default
                  splitrule = "gini", # gini index
                  seed = 2021, # ensure same bagging for all mtry
                  probability = TRUE, # to get probability estimates as the prediction value
                  importance = "impurity") # to measure feature importance
```
```{r}
rf.p_hat = predict(final.rf, Test, type = "response")

rf.p_hat = rf.p_hat$predictions[, 2] # probability estimate of being toxic
```
```{r}
hist(rf.p_hat,
     main="Probability Estimates of Mushroom Toxicity",
     xlab="Probability Estimate",
     ylab = "Frequency",
     col="blue",
     border="pink",
     breaks = 30)
```
As we can see above, the random forest model predicts relatively evenly between the two classes, with apparently very little uncertainty based on the absence of "middle ground" estimates.

Feature importance in Random Forest models is determined based on how much each feature reduces impurity in the classification process. Impurity, often measured using metrics like Gini impurity, quantifies the disorder or uncertainty in a set of data points regarding their classification. Therefore, features with higher importance values indicate greater reductions in impurity, signifying their stronger influence on accurately categorizing mushrooms as poisonous or edible.

```{r}
# Get feature importance
importance_rf <- importance(final.rf)

# Convert named numeric vector to data frame
importance_rf_df <- data.frame(Feature = names(importance_rf), Importance = importance_rf)

# Sort the data frame by Importance in descending order
importance_rf_sorted <- importance_rf_df[order(importance_rf_df$Importance, decreasing = TRUE), ]

# Print sorted feature importance
head(importance_rf_sorted, 10)
```

Interestingly, our 3 quantitative predictors were the most important features for the random forest decision trees. This could be due to there being many different levels which allow it to discriminate well on our training data. We would want to ensure it is not overfitting moving forward.

Finally, we can generate a preliminary accuracy score for this model using its hard classifications. Using a probability of 0.54 as our threshold, we can calculate the model's accuracy.
```{r}
rf.class = as.numeric(rf.p_hat > 0.54)

TP = sum(rf.class == 1 & rf.class == Test$class_p)
TN = sum(rf.class == 0 & rf.class == Test$class_p)

FP = sum(rf.class == 1 & rf.class != Test$class_p)
FN = sum(rf.class == 0 & rf.class != Test$class_p)

rf.accuracy = (TP + TN) / (TP + TN + FP + FN)

rf.accuracy
```
Using the test set, this model has an accuracy of 99.96%, which is an almost perfect accuracy score. 

## Model 3: Support Vector Machine

We normalized the predictors before building the SVM model to make sure all features had an equal impact. By putting them on the same scale, we avoided letting any one feature dominate the model. This helps the SVM make fair decisions and gives reliable results, which aligns with our goal of accurate predictions.

```{r}
# Identify quantitative variable names
quantitative_vars <- c("cap_diameter", "stem_height", "stem_width")

# Normalize quantitative variables in the training and test sets
preprocess_method <- preProcess(Train[, quantitative_vars], method = c("range"))

# Apply normalization to training set
Train_normalized <- predict(preprocess_method, Train[, quantitative_vars])

# Apply normalization to test set
Test_normalized <- predict(preprocess_method, Test[, quantitative_vars])

# Combine normalized quantitative variables with other predictors in the training and test sets
Train_processed <- cbind(Train_normalized, Train[, -which(names(Train) %in% quantitative_vars)])
Test_processed <- cbind(Test_normalized, Test[, -which(names(Test) %in% quantitative_vars)])
```

In our mushroom classification project, we incorporated Support Vector Machine (SVM) modeling with probability estimates using Platt scaling. Unlike logistic regression and decision trees, SVM initially doesn't produce probabilities directly. However, by setting the probability parameter to TRUE during model fitting, we enabled SVM to provide probability estimates. This is crucial for understanding the confidence of our predictions. Leveraging Platt scaling, we obtained these estimates, allowing us to gauge the likelihood of a mushroom belonging to a particular class, enhancing the interpretability and reliability of our classification results.

```{r}
# Fit SVM model with probability estimates using Platt scaling
svm_model <- svm(class_p ~ ., data = Train_processed, probability = TRUE)

# Predict probabilities for the test set
svm_probs <- attr(predict(svm_model, newdata = Test_processed, probability = TRUE), "probabilities")[,1]  
```

After developing the model we would like to examine the distribution of our predictions.

```{r}
hist(svm_probs,
     main="Probability Estimates of Mushroom Toxicity",
     xlab="Probability Estimate",
     ylab = "Frequency",
     col="blue",
     border="pink",
     breaks = 30)
```

Despite using Platt scoring to develop probabilities, we find the the SVM model is very confident in its predictions which results in a distribution which resembles binary classification.

```{r}
# Extract support vectors and coefficients
support_vectors <- svm_model$SV
coefficients <- t(svm_model$coefs) %*% svm_model$SV

# Create a data frame with support vectors and coefficients
support_vectors_and_coefficients <- data.frame(SupportVector = 1:nrow(support_vectors), Coefficient = coefficients)
head(support_vectors_and_coefficients, 1)
```

After analyzing the support vectors and their coefficients we can look at their magnitude and sign to gain insight. The greater the coefficient the greater the importance of the variable in the SVM model. A positive sign increases the likelihood of a mushroom being poisonous and a negative sign decreases the likelihood of a mushroom being positive. For the SVM model some of the most importsnt features were habitat, stem color, cap color, and gill color. 

We can generate a preliminary accuracy score for this model using its hard classifications. Using a probability of 0.54 as our threshold, we can calculate the model's accuracy.

```{r}
# Convert probabilities to class predictions based on a threshold (e.g., 0.5)
svm_class <- as.numeric(svm_probs > 0.54)

# Calculate True Positives (TP), True Negatives (TN), False Positives (FP), False Negatives (FN)
TP <- sum(svm_class == 1 & svm_class == Test_processed$class_p)
TN <- sum(svm_class == 0 & svm_class == Test_processed$class_p)
FP <- sum(svm_class == 1 & svm_class != Test_processed$class_p)
FN <- sum(svm_class == 0 & svm_class != Test_processed$class_p)

# Calculate accuracy
svm.accuracy <- (TP + TN) / (TP + TN + FP + FN)

svm.accuracy
```

Like our random forest model, the SVM model also has a very high accuracy score of 99.9% using a basic threshold of 0.54

## Model 4: Ensemble

In this code, we are attempting to implement a linear stacking model to improve the accuracy of our mushroom toxicity classification. It combines predictions from three base models (logistic regression, random forest, and support vector machine) in an effort to build an ensemble model that can hopefully provide even greater certainty to the classification. The stacking model is trained using logistic regression, and its predictions are evaluated for accuracy. This approach leverages the strengths of multiple models to create a more robust classification framework for mushroom foragers.

```{r}
# Create a new dataframe with predicted probabilities from base models and original features
stacking_data <- cbind(lr.p_hat, rf.p_hat, svm_probs, Test_processed)

# Train a linear model (e.g., logistic regression) using the predicted probabilities and original features
stacking_model <- glm(class_p ~ lr.p_hat + rf.p_hat + svm_probs, data = stacking_data, family = binomial)

summary(stacking_model)

# Predict probabilities for the test set using the stacking model
stacking_probs <- predict(stacking_model, newdata = stacking_data, type = "response")

# Convert probabilities to class predictions based on a threshold (e.g., 0.5)
stacking_class <- as.numeric(stacking_probs > 0.54)

# Calculate accuracy
TP <- sum(stacking_class == 1 & stacking_class == Test_processed$class_p)
TN <- sum(stacking_class == 0 & stacking_class == Test_processed$class_p)
FP <- sum(stacking_class == 1 & stacking_class != Test_processed$class_p)
FN <- sum(stacking_class == 0 & stacking_class != Test_processed$class_p)

en.accuracy <- (TP + TN) / (TP + TN + FP + FN)

en.accuracy
```

Unsurprisingly, since the ensemble leveraged 2 very strong models (the random forest and the SVM models), it was able to make strong predictions. The overall accuracy was the same as the extremely high RF model despite the less accurate logistic regression model. Moving forward we can see that the LR predictor is insignificant and may only add noise to predictions on unseen data thus it may be best to not include it in the ensemble in the application setting.

```{r}
# Train a linear model (e.g., logistic regression) using the predicted probabilities and original features
stacking_model <- glm(class_p ~ rf.p_hat + svm_probs, data = stacking_data, family = binomial)

summary(stacking_model)

# Predict probabilities for the test set using the stacking model
stacking_probs <- predict(stacking_model, newdata = stacking_data, type = "response")

# Convert probabilities to class predictions based on a threshold (e.g., 0.5)
stacking_class <- as.numeric(stacking_probs > 0.54)

# Calculate accuracy
TP <- sum(stacking_class == 1 & stacking_class == Test_processed$class_p)
TN <- sum(stacking_class == 0 & stacking_class == Test_processed$class_p)
FP <- sum(stacking_class == 1 & stacking_class != Test_processed$class_p)
FN <- sum(stacking_class == 0 & stacking_class != Test_processed$class_p)

en.accuracy <- (TP + TN) / (TP + TN + FP + FN)

en.accuracy
```

```{r}
hist(stacking_probs,
     main="Probability Estimates of Mushroom Toxicity",
     xlab="Probability Estimate",
     ylab = "Frequency",
     col="blue",
     border="pink",
     breaks = 30)
```

# Model Evaluation

Now that all four models have been built and optimized, we can conduct a more thorough evaluation of each model and how they compare to each other. This analysis will inform our our discussion and subsequent conclusions regarding the best method to classify mushrooms for foragers.

## Accuracy

In the model development section, we conducted a preliminary analysis of the models using accuracy as the metric. We can now view all of the accuracies of the finalized models here.

```{r}
accuracy.table = tibble(model = c("Logistic Regression",
                                  "Random Forest",
                                  "Support Vector Machine",
                                  "Ensemble"
                                  ),
                        accuracy = c(lr.accuracy,
                                     rf.accuracy,
                                     svm.accuracy,
                                     en.accuracy
                                     ))

accuracy.table
```

While accuracy is a nice preliminary analysis of the model's performance due to its simplicity and efficiency, it is a poor metric to use for more meaningful evaluation of the models. Accuracy does not account for the fact that there are different costs that exist depending on the type of mistake made in a given scenario. In our example, the cost of misclassifying a toxic mushroom can lead to death, whereas the cost of misclassifying a nontoxic mushroom is much less. Additionally, accuracy is unable to consider the nuances between different probability estimations, as it classifies a probability of 0.55 and 0.99 the same, given our threshold is 0.54. For these reasons, we need a more robust method to evaluate the models.

## Cost

Analyzing the models using cost as a metric can resolve many of the issues associated with using accuracy as a metric. The cost of a false positive versus the cost of a false negative are much different from each other when we consider the context of our scenario. A false positive would occur if a nontoxic mushroom is incorrectly classified as a toxic mushroom. A false negative would occur if a toxic mushroom is incorrectly classified as a nontoxic mushroom. Intuitively, the cost of a false positive would be much less than the cost of a false negative, as a false positive would lead to a forager refusing to eat a nontoxic mushroom and only incur the cost of wasted time foraging. A false negative, on the other hand, could result in a forager consuming a toxic mushroom and getting ill or even dying as a result. For these reasons, we decided to estimate the cost of a false positive as 1, and the cost of a false negative as 10.

Using these estimated costs, we can conduct a misclassification analysis for each of the models and compare the results.

```{r}
# truth: {0,1} vector
# score: risk score with larger values correspond to label = 1.
# thres: vector of thresholds at which to calculate metric.
# Note: decision is 1 if score > thres, 0 if score <= thres.
perf_table <- function(truth, score, thres){
  x = thres %>% unique() %>% sort()
  
  RESULTS = tibble()
  
  for (threshold in x) {
    class = as.numeric(score > threshold)
    
    TP = sum(class == 1 & class == truth)
    TN = sum(class == 0 & class == truth)
    
    FP = sum(class == 1 & class != truth)
    FN = sum(class == 0 & class != truth)
    
    out = tibble(threshold, TP, TN, FP, FN)
    
    RESULTS = bind_rows(RESULTS, out)
  }
  
  return(RESULTS)
}
```

*Logistic Regression:*
```{r}
thresholds = seq(0, 1, length = 1000)
lr.perf = perf_table(truth = Test$class_p, score = lr.p_hat, thres = thresholds)
head(lr.perf)
```
```{r}
lr.costs = 1*lr.perf$FP + 10*lr.perf$FN

min.idx = which.min(lr.costs)

ggplot() +
  geom_line(aes(x= lr.perf$threshold, y= lr.costs)) +
  geom_vline(xintercept = 1 / (1 + 10), col = "purple") +
  geom_point(aes(x=lr.perf$threshold[min.idx], 
                 y= min(lr.costs)), colour="orange") + 
  labs(title = "Logistic Regression: Cost of FP = 1 and FN = 10",
       x = "threshold",
       y = "Cost")
```
Above is the cost analysis of the logistic regression model. The estimated optimal threshold is indicated by the orange point, while the theoretical optimal threshold is indicated by the purple line. Here we can see that to minimize costs our optimal threshold is actually lower than the theoretically optimal threshold of .09.

```{r}
lr.perf[which.min(lr.costs), ]

min(lr.costs)
```

This model turns out to be a relatively poor model in practice, especially compared to our other models. However, it is able to somewhat effectively reduce the the number of costly false negatives by setting a threshold of .05 which is indeed lower than our theoretical ideal threshold. When you factor in the the 14 false negatives and the 2,946 false positives the overall cost amounts to 3,086.

*Random Forest:*
```{r}
thresholds = seq(0, 1, length = 1000)
rf.perf = perf_table(truth = Test$class_p, score = rf.p_hat, thres = thresholds)

rf.costs = 1*rf.perf$FP + 10*rf.perf$FN

min.idx = which.min(rf.costs)

ggplot() +
  geom_line(aes(x= rf.perf$threshold, y= rf.costs)) +
  geom_vline(xintercept = 1 / (1 + 10), col = "purple") +
  geom_point(aes(x=rf.perf$threshold[min.idx], 
                 y= min(rf.costs)), colour="orange") +
  labs(title = "Random Forest: Cost of FP = 1 and FN = 10",
       x = "threshold",
       y = "Cost")
```
Above is the cost analysis of the random forest model. We can see in this plot the optimal estimated threshold is notably higher than the theoretical threshold. Given we have sufficient data for our model, the estimated threshold is preferred over the theoretical one.

```{r}
rf.perf[which.min(rf.costs), ]

min(rf.costs)
```
At a threshold of 0.266, the cost is minimized at a value of 11 with 0 false negatives and 11 false positives. This performance is ideal, as it eliminates the costly false negatives and minimizes the number of false positives. In practice this performance is also ideal because there are no times that a toxic mushroom is incorrectly classified as nontoxic and accidentally ingested by a forager.

*Support Vector Machine:*

```{r}
thresholds = seq(0, 1, length = 1000)
svm.perf = perf_table(truth = Test$class_p, score = svm_probs, thres = thresholds)

svm.costs = 1*svm.perf$FP + 10*svm.perf$FN

min.idx = which.min(svm.costs)

ggplot() +
  geom_line(aes(x= svm.perf$threshold, y= svm.costs)) +
  geom_vline(xintercept = 1 / (1 + 10), col = "purple") +
  geom_point(aes(x=svm.perf$threshold[min.idx], 
                 y= min(svm.costs)), colour="orange") +
  labs(title = "Support Vector Machine: Cost of FP = 1 and FN = 10",
       x = "threshold",
       y = "Cost")
```

Above is the cost analysis of the SVM model. We can see in this plot the optimal estimated threshold is lower than the theoretical threshold. Given we have sufficient data for our model, the estimated threshold is preferred over the theoretical one.

```{r}
svm.perf[which.min(svm.costs), ]
min(svm.costs)
```

At a threshold of 0.055, the cost is minimized at a value of 15 with 0 false negatives and 15 false positives. This performance is once again ideal, as there are no costly false negatives and the number of false positives is kept relatively low. Once again, this model ensures that a forager is not likely to take a risk on any poisonous mushrooms that could lead to adverse effects. Instead the only cost is the lost time from abandoning a potentially safe mushroom.

*Ensemble:*

```{r}
thresholds = seq(0, 1, length = 1000)
en.perf = perf_table(truth = Test$class_p, score = stacking_probs, thres = thresholds)

en.costs = 1*en.perf$FP + 10*en.perf$FN

min.idx = which.min(en.costs)

ggplot() +
  geom_line(aes(x= en.perf$threshold, y= en.costs)) +
  geom_vline(xintercept = 1 / (1 + 10), col = "purple") +
  geom_point(aes(x=en.perf$threshold[min.idx], 
                 y= min(en.costs)), colour="orange") +
  labs(title = "Ensemble: Cost of FP = 1 and FN = 10",
       x = "threshold",
       y = "Cost")
```

Above is the cost analysis of the ensemble model. Very similar to the SVM model, we can see in this plot the optimal estimated threshold is once again lower than the theoretical threshold. Given we have sufficient data for our model, the estimated threshold is preferred over the theoretical one.

```{r}
en.perf[which.min(en.costs), ]
min(en.costs)
```

At a threshold of 0.053 for the ensemble model, the cost can be reduced all the way to a value of 2 with 0 false negatives and 2 false positives. This performance is the strongest yet with 0 of the potentially deadly false negatives and a mere 2 false positives. This shows that the ensemble model is using both of our stronger models (RF and SVM) to build a superior classification model. This demonstrates the potential value of an ensemble model. Although it had the same baseline accuracy as the RF model, when applied ti a practical setting with costs considered it showed its potential merit being greater than any one model. Granted, there is a risk of overfiting since the ensemble model was trained on the test data since we first had to fit the component models on the training data. But if we can assume that our test data is representative of mushroom population then the ensemble model is valuable.

# Discussion

Do you think a hard classification or probability/score is better for this scenario? -> probability is better! people can make their own judgments on what probability they are comfortable with, foragers prone to more risky behavior could have a lower FN cost and therefore be more willing to take their chances, there are plenty of mushrooms that are deemed toxic but will ultimately only give you a stomachache so the situations are not necessarily life or death

In evaluating various machine learning models such as logistic regression, random forest, and support vector machine, it became evident that ensemble models, such as stacking, outperformed individual algorithms in terms of accuracy and cost analysis. This finding suggests that combining the strengths of different algorithms can lead to improved performance in classifying mushroom toxicity.

Analyzing the coefficients of logistic regression or support vectors or the feature importances of tree-based models like random forest revealed the most influential features in predicting mushroom toxicity. Notably, features related to cap shape, odor, gill size, and habitat were found to have significant importance in distinguishing between toxic and non-toxic mushrooms. Noting the most important features is useful for foragers, so they can direct their attention to the most important features.

Experimenting with different classification thresholds allowed for the analysis of the trade-off between false positives and false negatives. For instance, lowering the threshold might reduce the risk of mistakenly classifying toxic mushrooms as safe (false negatives), but it could increase the likelihood of incorrectly labeling safe mushrooms as toxic (false positives). Finding the optimal threshold involves considering the consequences of misclassification in the context of mushroom foraging safety. This is an important consideration for foragers who may have different risk tolerances. In practice, a hunter and gatherer who subsists this way may have a greater cost for false positives since he doesn't have as many alternatives, but a hobbyist may have a lower cost for false positives, because they likely have alternatives readily available at home.

# Conclusion

*Ensemble Model Superiority:* The ensemble model, combining the strengths of random forest and support vector machine models, outperformed individual models in minimizing the cost of misclassification, demonstrating the potential value of ensemble methods in mushroom toxicity classification.

*Cost-Aware Thresholding:* Implementing cost-aware thresholding, where the threshold for classifying toxic mushrooms is adjusted based on the costs associated with false positives and false negatives, significantly improved the practical utility of the models, especially in scenarios where the cost of misclassification is asymmetric.

*Probability Estimates for Flexibility:* Probability estimates provide greater flexibility for decision-making in real-world scenarios, allowing foragers to make informed decisions based on their risk tolerance. This approach acknowledges the uncertainty inherent in mushroom classification and empowers foragers to assess the probability of toxicity according to their own preferences and risk factors.

# Limitations

*Missing Data Handling:* Removing columns with a large percentage of missing values may lead to loss of potentially valuable information, especially if those features could have been useful for classification.

*Normalization Assumptions:* Normalizing quantitative variables assumes that they have a linear relationship with the target variable, which may not always hold true.

*Model Selection:* Although logistic regression, random forest, support vector machine, and ensemble models were implemented, other models like gradient boosting or neural networks could also be explored for potentially better performance.

*Threshold Selection:* The choice of threshold for classification affects the model's performance, and selecting an optimal threshold can be challenging, especially considering the trade-off between false positives and false negatives.

*Generalizability:* The performance of the models may vary when applied to different datasets or mushroom populations, and their generalizability to real-world foraging scenarios needs to be validated.

*Assumptions of Cost Estimation:* The assumed costs of false positives and false negatives may not accurately reflect the real-world consequences of misclassification in mushroom foraging.
