---
title: "SYS 6018: Data Mining Final Project" 
author: "Lea Jih-Vieira, Mitch Whalen"
format: sys6018hw-html
---

```{r config, include=FALSE}
# Set global configurations and settings here
knitr::opts_chunk$set()                 # set global chunk options
ggplot2::theme_set(ggplot2::theme_bw()) # set ggplot2 theme
```

# Required R packages {.unnumbered .unlisted}

```{r packages, message=FALSE, warning=FALSE}
library(tidyverse) # functions for data manipulation  
library(ggplot2)
library(fastDummies)
library(ranger)
library(janitor)
```

# Introduction

Many people enjoy mushroom foraging as a hobby and a lifestyle, hunting through woods to find ingredients for their next meal. However, foraging for mushrooms is not a casual activity by any means. Foraging requires vast knowledge of the wild mushrooms that grow in a given area and how to identify them ([Incredible Mushrooms](https://www.incrediblemushrooms.com/wild-mushroom-foraging.html)). It is imperative that a forager can tell the difference between an edible mushroom and a poisonous one, especially when the poisonous varieties can look almost identical to their edible counterparts ([Incredible Mushrooms](https://www.incrediblemushrooms.com/wild-mushroom-foraging.html)).

This report aims to provide foragers a model-based method of determining the probability that a foraged mushroom is poisonous or not. To accomplish this, we will utilize a dataset of macromorphological mushroom characteristics that a forager could observe without the need for more advanced technologies. By providing a model-based method to determine the probability of mushroom toxicity using easily observable features, we hope to provide foragers a more reliable and accurate method to identify mushrooms.

# Load Data

```{r, include = F}
df = read_csv2("secondary_data.csv", col_names = TRUE)
```

```{r}
head(df)
```

# Data Description

```{r}
# View the structure of the data
str(df)
```
Our data set has 61,069 instances of 21 variables.

*Response Variable:*
**class**: Binary class divided in edible=e and poisonous=p (with the latter one also containing mushrooms of unknown edibility).

*Predictor Variables:*
**cap-diameter**: Diameter of the mushroom cap in centimeters (quantitative).
**cap-shape**: Shape of the mushroom cap, categorized as bell, conical, convex, flat, sunken, spherical, or other (categorical).
**cap-surface**: Surface texture of the mushroom cap, categorized as fibrous, grooves, scaly, smooth, shiny, leathery, silky, sticky, wrinkled, fleshy, or other (categorical).
**cap-color**: Color of the mushroom cap, categorized as brown, buff, gray, green, pink, purple, red, white, yellow, blue, orange, black, or other (categorical).
**does-bruise-bleed**: Indicates whether the mushroom bruises or bleeds when damaged, categorized as bruises or bleeding (yes) or no (no) (categorical).
**gill-attachment**: Attachment of gills to the stem, categorized as adnate, adnexed, decurrent, free, sinuate, pores, none, or unknown (categorical).
**gill-spacing**: Spacing between gills, categorized as close, distant, or none (categorical).
**gill-color**: Color of the mushroom gills, similar to cap color, with additional category none (categorical).
**stem-height**: Height of the mushroom stem in centimeters (quantitative).
**stem-width**: Width of the mushroom stem in millimeters (quantitative).
**stem-root**: Root structure of the mushroom stem, categorized as bulbous, swollen, club, cup, equal, rhizomorphs, rooted, or unknown (categorical).
**stem-surface**: Surface texture of the mushroom stem, similar to cap surface, with additional category none (categorical).
**stem-color**: Color of the mushroom stem, similar to cap color, with additional category none (categorical).
**veil-type**: Type of veil covering the gills, categorized as partial or universal (categorical).
**veil-color**: Color of the veil, similar to cap color, with additional category none (categorical).
**has-ring**: Indicates whether the mushroom has a ring around the stem, categorized as ring (yes) or none (no) (categorical).
**ring-type**: Type of ring around the stem, categorized as cobwebby, evanescent, flaring, grooved, large, pendant, sheathing, zone, scaly, movable, none, or unknown (categorical).
**spore-print-color**: Color of the mushroom spore print, similar to cap color (categorical).
**habitat**: Habitat where the mushroom grows, categorized as grasses, leaves, meadows, paths, heaths, urban, waste, or woods (categorical).
**season**: Season in which the mushroom is typically found, categorized as spring, summer, autumn, or winter (categorical).


```{r}
colSums(is.na(df))
```
We can confirm there are no missing values in the dataset, thus we do not need to remove rows or impute any values

# Data Cleaning

## Missing Values

To begin with the data cleaning we will examine the data set by checking for missing values

```{r}
# Check for missing values
colSums(is.na(df))
```

Some of the columns (ex. stem-root) are missing a large percentage of values. Since it would be difficult to identify these features in a practical setting such as observing a mushroom in the wild, we will remove any columns that are missing more than 1/3 of the values for that variable. This ensures that the model is interpretable and the features deemed most important will be identifiable.

```{r}
# Threshold for removing columns with more than 50% missing values
threshold <- nrow(df) * 0.33

# Identify columns with missing values exceeding the threshold
missing_cols <- colSums(is.na(df)) > threshold

# Remove columns with more than 50% missing values
cleaned_df <- df[, !missing_cols]

colSums(is.na(cleaned_df))
```
For the remaining rows, given our abundance of data, we will just remove any rows that contain missing values. If we were to have a smaller dataset we may consider mode or predictive imputation to handle the missing values.

```{r}
# Remove rows with missing values
cleaned_df <- na.omit(cleaned_df)

colSums(is.na(cleaned_df))
```
## Quantitative Predictors

First, we must convert our quantitative variables to a numeric type

```{r}
# List of quantitative variable names
quantitative_vars <- c("cap-diameter", "stem-height", "stem-width")
# Add all the quantitative variable names you want to convert to this list

# Loop through each variable name and convert it to numeric
for (var in quantitative_vars) {
  cleaned_df[[var]] <- as.numeric(cleaned_df[[var]])
}

# Check the structure of your dataframe after conversion
str(cleaned_df)
```

Now we will visualize our 3 quantitative predictors to check for skewness, outliers, and other elements which could affect our ability to construct useful models.

```{r, echo=FALSE}
# Histogram for "cap-diameter"
ggplot(cleaned_df, aes(x = `cap-diameter`)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(x = "Cap Diameter", y = "Frequency", title = "Histogram of Cap Diameter")

# Histogram for "stem-height"
ggplot(cleaned_df, aes(x = `stem-height`)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(x = "Stem Height", y = "Frequency", title = "Histogram of Stem Height")

# Histogram for "stem-width"
ggplot(cleaned_df, aes(x = `stem-width`)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  labs(x = "Stem Width", y = "Frequency", title = "Histogram of Stem Width")
```

These histograms show a clear skew in the data and some outliers in the upper tail. For certain models like regression models this could be impactful and is worth considering.

## Categorical Predictors

We will now visualize our categorical variables.

```{r, echo=FALSE}

# Function to create and display a bar plot for each logical and character variable
create_and_display_bar_plot <- function(df, var) {
  for (v in var) {
    if (is.logical(df[[v]]) || is.character(df[[v]])) {
      # Count frequency of each category
      freq_table <- table(df[[v]], useNA = "ifany")
      
      # Convert frequency table to dataframe
      freq_df <- as.data.frame(freq_table)
      names(freq_df) <- c("Category", "Frequency")
      
      # Create bar plot
      plot <- ggplot(freq_df, aes(x = Category, y = Frequency, fill = Category)) +
        geom_bar(stat = "identity") +
        labs(x = v, y = "Frequency", title = paste("Bar Plot of", v)) +
        theme(axis.text.x = element_text(angle = 45, hjust = 1))
      
      # Display plot
      print(plot)
    }
  }
}

# Create and display bar plots for logical and character variables
create_and_display_bar_plot(cleaned_df, names(cleaned_df))
```

One important note from these visualizations is that our class variable is roughly balanced, so we will not need to make adjustment for an imbalanced data set.

Next, we need to create dummy variables for the categorical variables. We will use one-hot encoding, so we can incorporate them into our model

```{r}
# Get logical and character variable names
logical_vars <- names(cleaned_df)[sapply(cleaned_df, is.logical)]
character_vars <- names(cleaned_df)[sapply(cleaned_df, is.character)]

# Perform one-hot encoding for logical and character variables
encoded_df <- dummy_cols(cleaned_df, select_columns = c(logical_vars, character_vars), remove_first_dummy = TRUE, remove_selected_columns = TRUE)

# Display the encoded dataframe
head(encoded_df)
```

# Model Development

## Train/Test Split

We will be implementing a 80/20 train/test split of the cleaned data. This will allow us to train the model on an adequate number of observations and also test our models on data it has not seen before.

```{r}
set.seed(2019)

train.idx = sample(nrow(encoded_df), # sample row indices
                   nrow(encoded_df)*0.8,# 80% of data
                   replace = FALSE)

Train = encoded_df[train.idx, ]
Train = clean_names(Train) # changes all - to _ in column names
Train$class_p = as.factor(Train$class_p)

Test = encoded_df[-train.idx, ]
Test = clean_names(Test) # changes all - to _ in column names
Test$class_p = as.factor(Test$class_p)
```

We will be implementing three different classification models: logistic regression, random forest, and support vector machine (SVM). In addition to these three models, we will be implementing an ensemble using up to these three models to see if the ensemble possesses better predictive power than the individual models.

## Model 1: Logistic Regression

We chose to use Logistic Regression as our first model because of its commonality to model relationships with binary dependent variables. Logistic Regression models are convenient because they are one of the easiest models to implement and train; however, they have more limitations and less flexibility compared to other classification models.

As we modeled in the data cleaning section, the numeric features of this data are heavily skewed and contain considerable outliers. Because this model type can be influenced by the presence of outliers, we chose to log transform those variables to lessen the effects of the potential outliers. Since we have observations where these values equal zero, we have to add a small constant to the numeric data to ensure we do not get values of infinity while log-transforming.

```{r}
num.features = c("cap_diameter", "stem_height", "stem_width")

Train.transformed = Train

for (column in num.features) {
  Train.transformed[ , column] = Train[ , column] + 2e-323 %>% log()
}
```

From here, we can fit a logistic regression model.

```{r}
fit_lr = glm(class_p ~ ., 
             family= 'binomial', 
             data= Train.transformed)
```

Using our logistic regression model, we can plot the probability estimates for the test data to observe the nature of the probability predictions.
```{r}
lr.p_hat= predict(fit_lr, Test, type= 'response') # response is the point estimate of the probability
```
```{r}
hist(lr.p_hat,
     main="Probability Estimates of Mushroom Toxicity",
     xlab="Probability Estimate",
     ylab = "Frequency",
     col="blue",
     border="pink",
     breaks = 30)
```
As we can see above, the model does not produce many "middle ground" probability estimates of mushroom toxicity. The estimates are either very close to zero or very close to one. It is notable that the model predicts a lot more nontoxic classifications as opposed to toxic. Later in our model evaluation analysis, we will be able to see how effective this model is in predicting mushroom toxicity, especially in situations where the penalty of misclassification is high.

Finally, we can generate a preliminary accuracy score for this model using its hard classifications. Using a probability of 0.5 as our threshold (the guessing average), we can calculate the model's accuracy.

```{r}
lr.class = as.numeric(lr.p_hat > 0.5)

TP = sum(lr.class == 1 & lr.class == Test$class_p)
TN = sum(lr.class == 0 & lr.class == Test$class_p)

FP = sum(lr.class == 1 & lr.class != Test$class_p)
FN = sum(lr.class == 0 & lr.class != Test$class_p)

lr.accuracy = (TP + TN) / (TP + TN + FP + FN)

lr.accuracy
```
The model produces a preliminary accuracy of 0.47, which is a little lower than the guessing average of 0.5 for a binary classification problem. Further analysis will be conducted in the model evaluation section later in this report.

## Model 2: Random Forest

The second model we wanted to explore is a tree-based model. Tree-based models are a powerful model to use because of their flexibility; they naturally include feature interactions and reduce the need of features transformations while still producing an intuitive model. Since we are trying to model a binary categorical outcome, we will need a classification tree that will allow us to estimate the probability of each class.

We chose to use a random forest model because it is generally known to be more robust to overfitting than its alternatives. The objective of this model is the minimize the loss function, which we will be using node impurity in our case. Node impurity refers to the probability for a random instance being misclassified when chosen randomly ([Towards Data Science](https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c)). There are many metrics to measure node impurity, but we have chosen to use Gini Index in our model. Gini Index implements a soft approximation of the Misclassification Error impurity measure. This method is beneficial in situations where we want to prioritize accuracy, which is the case in our scenario. We want to be as accurate as possible in our mushroom toxicity predictions because even one mistake could be fatal to a forager.

We decided to run an out-of-bag analysis of the random forest to assess performance and identify the optimal tuning parameters. The default values from the `ranger()` function uses `mtry = sqrt(p) =` `r sqrt(ncol(Train)-1) %>% floor()` and `min.bucket=1`. Given the size of our data, however, we are restricted in the number of analyses and values we can test before computing power becomes a limitation. We ultimately decided to test `mtry` values between 4 and 16, and `min.bucket` values between 1 and 8. To evaluate the performance of each model, we used the `prediction.error` attribute from the model; for classification models, it produces the accuracy value of the model. In other words, the proportion of misclassified observation are reported by the `prediction.error` attribute.

```{r}
#: grid for tuning
tune_grid = expand_grid(
  mtry = seq(4, 16, by = 2),
  min.bucket = seq(1, 8, by = 2)
)

# OOB analysis
ACCURACY = tibble() # initiate results df
for(i in 1:nrow(tune_grid)) {
  mtry = tune_grid$mtry[i] # pull mtry value for this iteration
  
  min.bucket = tune_grid$min.bucket[i] # pull min-bucket value for this iteration
  
  rf = ranger(class_p~ ., 
              data = Train,
              mtry = mtry, # changes each iteration
              min.bucket = min.bucket, # changes each iteration
              num.trees = 500, # default
              splitrule = "gini", # gini index
              seed = 2021) # ensure same bagging for all mtry
  
  out = tibble(mtry, min.bucket,
               accuracy = rf$prediction.error)
  
  ACCURACY = bind_rows(ACCURACY, out)
}
```
```{r}
#-- Aggregate Results
ACCURACY_agg = ACCURACY %>% 
  group_by(mtry, min.bucket) %>% 
  summarize(n=n(), accuracy = accuracy, .groups="drop")
```
```{r}
#-- Plot Results
ACCURACY_agg %>% 
  mutate(min.bucket = factor(min.bucket)) %>% # make into factor for plotting
  ggplot(aes(mtry, accuracy, color=min.bucket)) + 
  geom_point() + geom_line()
```
Above we can see a visualization reporting the performance of each combination of `mtry` and `min.bucket` values run. This simulation has shown us that the default value `min.bucket = 1` and `mtry = 14` gives the best prediction for our data because it minimizes the prediction error the most.

Finally, we can test our model on the test set using our optimal tuning parameters.
```{r}
final.rf = ranger(class_p~ ., 
                  data = Train,
                  mtry = 14, # optimal mtry
                  min.bucket = 1, # optimal min.bucket
                  num.trees = 500, # default
                  splitrule = "gini", # gini index
                  seed = 2021, # ensure same bagging for all mtry
                  probability = TRUE) # to get probability estimates as the prediction value
```
```{r}
rf.p_hat = predict(final.rf, Test, type = "response")

rf.p_hat = rf.p_hat$predictions[, 2] # probability estimate of being toxic
```
```{r}
hist(rf.p_hat,
     main="Probability Estimates of Mushroom Toxicity",
     xlab="Probability Estimate",
     ylab = "Frequency",
     col="blue",
     border="pink",
     breaks = 30)
```
As we can see above, the random forest model predicts relatively evenly between the two classes, with a few more "middle ground" probability estimates as compared to the logistic regression model.

Finally, we can generate a preliminary accuracy score for this model using its hard classifications. Using a probability of 0.5 as our threshold, we can calculate the model's accuracy.
```{r}
rf.class = as.numeric(rf.p_hat > 0.5)

TP = sum(rf.class == 1 & rf.class == Test$class_p)
TN = sum(rf.class == 0 & rf.class == Test$class_p)

FP = sum(rf.class == 1 & rf.class != Test$class_p)
FN = sum(rf.class == 0 & rf.class != Test$class_p)

rf.accuracy = (TP + TN) / (TP + TN + FP + FN)

rf.accuracy
```
Using the test set, this model has an accuracy of 99.9%, which is an almost perfect accuracy score. 

## Model 3: Support Vector Machine
- logistic and trees produce probabilities, SVM doesn't
- can make SVM into a probability using plat score (??) or something
- just need to be careful if ensembling because not all the same scale

```{r}
# model 3 code here
```

## Model 4: Ensemble

- can also discuss maybe only need one model versus ensemble

```{r}
# model 4 code here
```

# Model Evaluation

Now that all four models have been built and optimized, we can conduct a more thorough evaluation of each model and how they compare to each other. This analysis will inform our our discussion and subsequent conclusions regarding the best method to classify mushrooms for foragers.

## Accuracy

In the model development section, we conducted a preliminary analysis of the models using accuracy as the metric. We can now view all of the accuracies of the finalized models here.

```{r}
accuracy.table = tibble(model = c("Logistic Regression",
                                  "Random Forest"#,
                                  #"Support Vector Machine",
                                  #"Ensemble"
                                  ),
                        accuracy = c(lr.accuracy,
                                     rf.accuracy#,
                                     #svm.accuracy,
                                     #en.accuracy
                                     ))

accuracy.table
```
COMPARISON HERE

While accuracy is a nice preliminary analysis of the model's performance due to its simplicity and efficiency, it is a poor metric to use for more meaningful evaluation of the models. Accuracy does not account for the fact that there are different costs that exist depending on the type of mistake made in a given scenario. In our example, the cost of misclassifying a toxic mushroom can lead to death, whereas the cost of misclassifying a nontoxic mushroom is much less. Additionally, accuracy is unable to consider the nuances between different probability estimatations, as it classifies a probability of 0.55 and 0.99 the same, given our threshold is 0.5. For these reasons, we need a more robust method to evaluate the models.

## Cost

Analyzing the models using cost as a metric can resolve many of the issues associated with using accuracy as a metric. The cost of a false positive versus the cost of a false negative are much different from each other when we consider the context of our scenario. A false positive would occur if a nontoxic mushroom is incorrectly classified as a toxic mushroom. A false negative would occur if a toxic mushroom is incorrectly classified as a nontoxic mushroom. Intuitively, the cost of a false positive would be much less than the cost of a false negative, as a false positive would lead to a forager refusing to eat a nontoxic mushroom and only incur the cost of wasted time foraging. A false negative, on the other hand, could result in a forager consuming a toxic mushroom and getting ill or even dying as a result. For these reasons, we decided to estimate the cost of a false positive as 1, and the cost of a false negative as 10.

Using these estimated costs, we can conduct a misclassification analysis for each of the models and compare the results.

```{r}
# truth: {0,1} vector
# score: risk score with larger values correspond to label = 1.
# thres: vector of thresholds at which to calculate metric.
# Note: decision is 1 if score > thres, 0 if score <= thres.
perf_table <- function(truth, score, thres){
  x = c(-Inf, thres, Inf) %>% unique() %>% sort() # expand and clean thresholds
  tibble(truth, score) %>%
    # create groups by threshold
    mutate(
      bin = findInterval(score, x, left.open = TRUE),
      val = x[bin+1]
    ) %>%
    # counts by group/threshold
    group_by(val) %>%
    summarize(n = n(), n.1 = sum(truth), n.0 = n-n.1) %>%
    ungroup() %>%
    complete(val = thres, fill = list(n=0L, n.1=0L, n.0 = 0L)) %>%
    
    # calculate metrics
    arrange(val) %>%
    mutate(
      TN = cumsum(n.0), # True negatives
      FN = cumsum(n.1), # False negatives
      TP = sum(n.1) - FN, # True positives
      FP = sum(n.0) - TN, # False positives
      TPR = TP/sum(n.1), # True positive rate (TP / Positives)
      FPR = FP/sum(n.0) # False positive rate (FP / Negatives)
    ) %>%
    # drop values outside of stated thresholds
    filter(val %in% thres) %>%
    # retain relevant metrics
    select(-n, -n.1, -n.0, score = val)
}
```
```{r}
thresholds = seq(0, 1, length = 1000)
perf = perf_table(truth = as.numeric(Test$class_p), score = lr.p_hat, thres = thresholds) %>%
  mutate(p_hat = score, gamma_hat = log(p_hat) - log(1-p_hat), .before=1) %>%
  select(-score)
```




# Discussion

Discuss results here (maybe can be combined with Model Evaluation)...

Do you think a hard classification or probability/score is better for this scenario? probability is better

# Conclusion

Conclusions here...

# Limitations

Maybe talk about limitations?? here...